{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc363103-aa55-4fd6-b372-4798fa4347ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c49fa89-0c81-4305-84e7-9419dd301240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from file for 5DL1 promoter \n",
    "data=np.load(\"science_data_5DL1.npy\")\n",
    "\n",
    "# Extract x and y data\n",
    "x_data = data[:, 0]\n",
    "y_data = data[:, 1]\n",
    "\n",
    "# Get unique x values and their corresponding indices\n",
    "unique_x, unique_indices = np.unique(x_data, return_index=True)\n",
    "\n",
    "# Extract unique y values based on unique indices\n",
    "unique_y = y_data[unique_indices]\n",
    "\n",
    "# Create unique data array with x and y values\n",
    "unique_data = np.column_stack((unique_x, unique_y))\n",
    "\n",
    "# Convert mean and variance data to torch tensors\n",
    "mean_data = torch.from_numpy(unique_data[:, 0]).double()\n",
    "var_data = mean_data * torch.from_numpy(unique_data[:, 1]).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b749ee-3ea1-4f45-a73b-3ed381408969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stoichiometric matrix defining the effect of each reaction on the system\n",
    "stoic_matrix = torch.tensor([[2.0, 0.0],    # Reaction 1: Promoter state goes from -1 to +1\n",
    "                             [0.0, 1.0],    # Reaction 2: mRNA is produced\n",
    "                             [-2.0, 0.0],   # Reaction 3: Promoter state goes from +1 to -1\n",
    "                             [0.0, -1.0]])  # Reaction 4: Degradation of mRNA\n",
    "\n",
    "def state_jump(reaction_index, stoic_matrix):\n",
    "    \"\"\"\n",
    "    Calculate state jump vector based on the selected reaction index and stoichiometry matrix, where, \n",
    "    state vector -> state vector + state jump vector.\n",
    "\n",
    "    Arguments:\n",
    "        reaction_index: Selected reaction index\n",
    "        stoic_matrix: Stoichiometry matrix\n",
    "\n",
    "    Returns:\n",
    "        State jump vector\n",
    "    \"\"\" \n",
    "    return torch.sum(stoic_matrix * (torch.exp(-b_inv * (reaction_index - torch.arange(stoic_matrix.shape[0]))**2)).view(-1, 1), dim=0)\n",
    "\n",
    "def reaction_selection(breaks, random_num):\n",
    "    \"\"\"\n",
    "    Select reaction based on the transition points and a random number. Transition points are \n",
    "    given by the ratio of cumulative sum of rates and the total rate.\n",
    "\n",
    "    Arguments:\n",
    "        breaks: Transition points between [0,1]\n",
    "        random_num: Random number in [0,1]\n",
    "\n",
    "    Returns:\n",
    "        Index of the next reaction\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.sigmoid(a_inv * (random_num - breaks)))\n",
    "\n",
    "def gillespie_simulation(poff_values, r, g):\n",
    "    \"\"\"\n",
    "    Perform differentiable Gillespie simulation for a 2-state promoter model.\n",
    "    \n",
    "    Arguments:\n",
    "        poff_values: Array of probabilities for promoter to be in OFF state. poff=koff/(kon+koff)\n",
    "        r: Rate of mRNA production.\n",
    "        g: Rate of mRNA degradation.\n",
    "        \n",
    "    Returns:\n",
    "        mean_final_states: Mean of the mRNA levels at the end of the simulation.\n",
    "        variances: Variance of the mRNA levels at the end of the simulation.\n",
    "    \"\"\"\n",
    "    # Initialize random seed for reproducibility\n",
    "    random_seed = torch.randint(1, 10000000, (1,))\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    mean_final_states = torch.empty(len(unique_data))\n",
    "    variances = torch.empty(len(unique_data))\n",
    "    \n",
    "    for n in range(len(unique_data)):\n",
    "        poff = poff_values[n].unsqueeze(0)\n",
    "        \n",
    "        final_states = 0.0\n",
    "        final_states_squared = 0.0\n",
    "\n",
    "        for j in range(num_simulations):\n",
    "            # Initial 'levels':\n",
    "            # The first component of 'levels' is the promoter state, initialized to -1\n",
    "            # The second component of 'levels' is the mRNA level, initialized to 0.\n",
    "            levels = torch.stack([torch.tensor(-1.0), torch.tensor(0.0)])\n",
    "            current_time = 0.0\n",
    "\n",
    "            while current_time < sim_time:\n",
    "                # Calculate reaction propensities\n",
    "                propensities = torch.stack([(1/poff-1.0) * torch.sigmoid(-c*levels[0]), \n",
    "                                            r * torch.sigmoid(-c*levels[0]), \n",
    "                                            torch.tensor([1.0]) * torch.sigmoid(c * levels[0]), \n",
    "                                            g * levels[1]])\n",
    "                propensities = torch.relu(propensities)\n",
    "\n",
    "                # Sum of all propensities\n",
    "                total_propensity = propensities.sum()\n",
    "\n",
    "                # Time until next reaction\n",
    "                dt = -torch.log(torch.rand(1)) / total_propensity\n",
    "                current_time += dt.item()\n",
    "\n",
    "                if current_time >= sim_time:\n",
    "                    break\n",
    "\n",
    "                # Determine which reaction occurs and update the system state\n",
    "                breaks = (propensities[:-1] / total_propensity).cumsum(dim=0)\n",
    "                reaction_index = reaction_selection(breaks, torch.rand(1))\n",
    "                levels = levels + state_jump(reaction_index, stoic_matrix)\n",
    "                levels[1] = torch.relu(levels[1])  # Ensure non-negative values for the mRNA number\n",
    "\n",
    "            # Accumulate mRNA level and its square\n",
    "            final_states += levels[1]\n",
    "            final_states_squared += levels[1] ** 2\n",
    "\n",
    "        # Calculate mean and variance of mRNA levels\n",
    "        mean_final_state = final_states / num_simulations\n",
    "        variance = final_states_squared / num_simulations - mean_final_state ** 2\n",
    "        \n",
    "        mean_final_states[n] = mean_final_state\n",
    "        variances[n] = variance\n",
    "\n",
    "    return mean_final_states, variances\n",
    "\n",
    "def loss_function(mean_final_states, variances):\n",
    "    \"\"\"\n",
    "    Loss function that calculates the mean squared error of the simulation results against data.\n",
    "\n",
    "    Arguments:\n",
    "        mean_final_states: Mean of the mRNA levels at the end of the simulation.\n",
    "        variances: Variance of the mRNA levels at the end of the simulation.\n",
    "        \n",
    "    Returns:\n",
    "        Loss value\n",
    "    \"\"\"\n",
    "    return torch.mean((mean_final_states - mean_data)**2 + (variances**0.5 - var_data**0.5)**2)\n",
    "\n",
    "# Define a function to write data to a file\n",
    "def write_to_file(filename, *args):\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(' '.join(map(str, args)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22aa0337-63e5-46d5-ba6a-bdae8df655a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 32.5766\n",
      "Epoch 10: Loss = 3.5539\n",
      "Epoch 20: Loss = 3.0240\n",
      "Epoch 30: Loss = 1.3619\n",
      "Epoch 40: Loss = 1.3730\n",
      "Epoch 50: Loss = 1.5025\n",
      "Epoch 60: Loss = 0.7192\n",
      "Epoch 70: Loss = 0.5266\n",
      "Epoch 80: Loss = 0.9514\n",
      "Epoch 90: Loss = 1.6064\n",
      "Epoch 100: Loss = 4.0549\n",
      "Epoch 110: Loss = 0.7572\n",
      "Epoch 120: Loss = 8.5706\n",
      "Epoch 130: Loss = 9.4422\n",
      "Epoch 140: Loss = 3.1611\n",
      "Epoch 150: Loss = 2.8473\n",
      "Epoch 160: Loss = 0.8806\n",
      "Epoch 170: Loss = 0.6561\n",
      "Epoch 180: Loss = 0.7741\n",
      "Epoch 190: Loss = 1.5545\n",
      "Epoch 200: Loss = 4.0650\n",
      "Epoch 210: Loss = 4.8601\n",
      "Epoch 220: Loss = 2.9063\n",
      "Epoch 230: Loss = 4.8462\n",
      "Epoch 240: Loss = 1.7498\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility \n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define simulation hyperparameters\n",
    "num_iterations = 250\n",
    "num_simulations = 200\n",
    "sim_time = 0.2\n",
    "a_inv = 200.0\n",
    "b_inv =20.0\n",
    "c = 20.0\n",
    "\n",
    "# Initialize parameters with random values\n",
    "poff_values = torch.nn.Parameter(torch.linspace(0.03, 0.97, len(unique_data)))\n",
    "r = torch.nn.Parameter((1e+2) * torch.rand(1))\n",
    "g = torch.nn.Parameter((1e+1) * torch.rand(1))\n",
    "\n",
    "# Define the Adam optimizer and include all parameters that require gradients\n",
    "optimizer = optim.Adam([poff_values, r, g], lr=0.1)\n",
    "\n",
    "# Define filenames for saving results\n",
    "filename1 = \"learning_science_5DL1_poff_250.txt\"\n",
    "if os.path.exists(filename1):\n",
    "    os.remove(filename1)   \n",
    "filename2 = \"learning_science_5DL1_250.txt\"\n",
    "if os.path.exists(filename2):\n",
    "    os.remove(filename2)\n",
    "\n",
    "# Main optimization loop\n",
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    # Forward differentiable Gillespie simulation\n",
    "    mean_final_states, variances = gillespie_simulation(poff_values, r, g)\n",
    "    \n",
    "    # Compute the loss for the current iteration\n",
    "    loss = loss_function(mean_final_states, variances)\n",
    "\n",
    "    # Zero the gradients to prepare for backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the gradient of the loss with respect to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients to prevent exploding gradients problem\n",
    "    torch.nn.utils.clip_grad_norm_([poff_values, r, g], max_norm=1.0)\n",
    "\n",
    "    # Update the parameters using the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clamp r and g to ensure they are within valid range\n",
    "    r.data = torch.clamp(r.data, min=1.0)\n",
    "    g.data = torch.clamp(g.data, min=1.0, max=r.item())\n",
    "    poff_values.data = torch.clamp(poff_values, min=0.01, max=0.98)\n",
    "    poff_values.data, _ = torch.sort(poff_values.data)\n",
    "\n",
    "    # Save the values of the parameters after each iteration\n",
    "    if iteration % 1 == 0:\n",
    "        write_to_file(filename2, iteration, r.item(), g.item(), r.item() / g.item(), loss.item())\n",
    "        write_to_file(filename1, poff_values.tolist(), loss.item())\n",
    "        \n",
    "    if iteration % 10 == 0:\n",
    "        print(f\"Epoch {iteration}: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5993fe-dffb-40e9-b2cc-9b4db197dc89",
   "metadata": {},
   "source": [
    "### Extract learned parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d7b6cbc-1fde-4cce-8de9-e809c306f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the row index with minimum loss value. \n",
    "data=np.loadtxt(\"learning_science_5DL1_250.txt\")\n",
    "row_index = np.argmin(data[:, -1])\n",
    "r=data[row_index,1]\n",
    "g=data[row_index, 2]\n",
    "\n",
    "# Path to poff_values\n",
    "file_path = 'learning_science_5DL1_poff_250.txt'\n",
    "\n",
    "# Initialize an empty list to store rows\n",
    "poff_values = []\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove the trailing number by splitting at the last space and taking the first part\n",
    "        list_str = line.rsplit(' ', 1)[0]\n",
    "        # Remove surrounding brackets\n",
    "        list_str = list_str.strip('[').strip(']')\n",
    "        # Split the remaining string by commas to form a list of strings\n",
    "        str_values = list_str.split(',')\n",
    "        # Convert strings to floats\n",
    "        row = [float(value) for value in str_values]\n",
    "        # Append the row to our data list\n",
    "        poff_values.append(row)\n",
    "  \n",
    "poff_values = poff_values[row_index][:len(unique_data)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada85eb-a5cc-4292-9164-c8ca718a8b4a",
   "metadata": {},
   "source": [
    "### Standard deviation of r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b06233a5-eae1-41ee-ab49-299f74e6bb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.49004575597655\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define simulation hyperparameters\n",
    "num_points = 50\n",
    "num_simulations = 200\n",
    "a_inv=200.0\n",
    "b_inv=20.0\n",
    "c=20.0\n",
    "sim_time = 0.2\n",
    "num_points2 = 50\n",
    "\n",
    "# Initialize lists to store loss\n",
    "loss_list = []\n",
    "\n",
    "# Generate a range of r values around the optimal value\n",
    "r_list = np.linspace(r - r * 0.8, r * 2, num_points)\n",
    "\n",
    "# Perform simulation for each r value\n",
    "for r_value in r_list:\n",
    "    # Perform Gillespie simulation\n",
    "    mean, var = gillespie_simulation(torch.tensor(poff_values), torch.tensor([r_value]), torch.tensor([g]))\n",
    "    # Calculate loss based on simulation results\n",
    "    loss = loss_function(mean, var)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "# Polynomial fitting\n",
    "degree = 6\n",
    "# Fit a polynomial curve to the loss data\n",
    "coeffs = np.polyfit(r_list, loss_list, degree)\n",
    "poly_func = np.poly1d(coeffs)\n",
    "# Calculate first and second derivatives of the polynomial curve\n",
    "first_derivative = np.polyder(poly_func, 1)\n",
    "second_derivative = np.polyder(poly_func, 2)\n",
    "\n",
    "# Define evaluation points for curvature analysis\n",
    "evaluation_points = np.linspace(r_list[0], r_list[-1], num_points2)\n",
    "\n",
    "# Evaluate curvature at each point\n",
    "curvatures = [second_derivative(point) for point in evaluation_points]\n",
    "\n",
    "# Compute the average curvature\n",
    "average_curvature = np.mean(curvatures)\n",
    "\n",
    "# Compute standard deviation based on average curvature\n",
    "std_r = 1 / np.sqrt(abs(average_curvature))\n",
    "\n",
    "# Print the computed standard deviation\n",
    "print(std_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee755947-546b-4b20-be0c-a51f77c477b6",
   "metadata": {},
   "source": [
    "### Standard deviation of gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f95fc1b-4679-4351-9fb4-896a179bbba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.233986533236835\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define simulation hyperparameters\n",
    "num_points = 50\n",
    "num_simulation = 200\n",
    "a_inv=200.0\n",
    "b_inv=20.0\n",
    "c=20.0\n",
    "sim_time = 0.2\n",
    "num_points2 = 50\n",
    "\n",
    "# Initialize lists to store loss\n",
    "loss_list = []\n",
    "\n",
    "# Generate a range of g values around the optimal value\n",
    "g_list = np.linspace(g-g*0.8, g*2, num_points)\n",
    "\n",
    "# Perform simulation for each g value\n",
    "for g_value in g_list:\n",
    "    mean, var = gillespie_simulation(torch.tensor(poff_values), torch.tensor([r]), torch.tensor([g_value]))\n",
    "    loss = loss_function(mean, var)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "# Polynomial fitting\n",
    "degree = 6\n",
    "# Fit a polynomial curve to the loss data\n",
    "coeffs = np.polyfit(g_list, loss_list, degree)\n",
    "poly_func = np.poly1d(coeffs)\n",
    "# Calculate first and second derivatives of the polynomial curve\n",
    "first_derivative = np.polyder(poly_func, 1)\n",
    "second_derivative = np.polyder(poly_func, 2)\n",
    "\n",
    "# Define evaluation points for curvature analysis\n",
    "evaluation_points = np.linspace(g_list[0], g_list[-1], num_points2)\n",
    "\n",
    "# Evaluate curvature at each point\n",
    "curvatures = [second_derivative(point) for point in evaluation_points]\n",
    "\n",
    "# Compute the average curvature\n",
    "average_curvature = np.mean(curvatures)\n",
    "\n",
    "# Compute standard deviation based on average curvature\n",
    "std_g = 1 / np.sqrt(abs(average_curvature))\n",
    "\n",
    "# Print the computed standard deviation\n",
    "print(std_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef87c4b-e987-47dc-95d4-d48a5eda2c92",
   "metadata": {},
   "source": [
    "### Standard deviation of r/gamma using propagation of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f50f4adf-ffc2-424b-8a00-6d20001bc2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2714915869659795\n"
     ]
    }
   ],
   "source": [
    "std_r_g = (r/g) * ((std_r/r)**2 + (std_g/g)**2)**0.5\n",
    "print(std_r_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd72d93-e55e-48a6-8215-dc88eaef5ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
